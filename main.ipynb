{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e72dd03d-daeb-4e9d-a0d2-a8bde00f7ac0",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4c1b5e-8191-4ffd-83eb-11e6ff0b6e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.optim import AdamW\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Please enter the PATH of your training data here\n",
    "df = pd.read_csv('test_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d9e8df-b10f-449f-a3a5-a90c90627194",
   "metadata": {},
   "source": [
    "The data should be cleaned such that it has the columns `cleaned_text` and `classification`.\n",
    "|...|cleaned_text|classification|...|\n",
    "|:---:|:---:|:---:|:---:|\n",
    "|$\\vdots$|review 1|label 1|$\\vdots$|\n",
    "|$\\vdots$|$\\vdots$|$\\vdots$|$\\vdots$|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d571f74-126e-4439-a4f3-8ce2f8bcb06b",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8511caca-2b8e-4bcf-86cb-adf135dd973c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_reviews(reviews, tokenizer, max_length=128):\n",
    "    \"\"\"\n",
    "    Tokenizes a list of reviews using the BERT tokenizer.\n",
    "\n",
    "    Args:\n",
    "        reviews (list): A list of text reviews.\n",
    "        max_length (int): The maximum sequence length for padding.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing input_ids, attention_mask, and token_type_ids.\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenize and encode the reviews\n",
    "    encoded_dict = tokenizer.batch_encode_plus(\n",
    "        reviews,\n",
    "        add_special_tokens=True,      # Add '[CLS]' and '[SEP]'\n",
    "        max_length=max_length,        # Pad and truncate all reviews\n",
    "        padding='max_length',         # Pad to the max_length\n",
    "        truncation=True,              # Truncate sequences to max_length\n",
    "        return_attention_mask=True,   # Return attention mask\n",
    "        return_tensors='pt',          # Return PyTorch tensors\n",
    "    )\n",
    "\n",
    "    return encoded_dict\n",
    "\n",
    "# Get the list of reviews\n",
    "reviews = df['cleaned_text'].tolist()\n",
    "\n",
    "# Tokenize the reviews\n",
    "encoded_data = tokenize_reviews(reviews,tokenizer)\n",
    "\n",
    "# You now have the encoded data ready for training a BERT model\n",
    "input_ids = encoded_data['input_ids']\n",
    "attention_mask = encoded_data['attention_mask']\n",
    "labels = df['classification'].values  # Get the labels as a NumPy array\n",
    "\n",
    "print(\"Input IDs shape:\", input_ids.shape)\n",
    "print(\"Attention Mask shape:\", attention_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68346cb3-dbef-4b63-bcc8-7b7d01da3fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(\n",
    "    input_ids, labels, random_state=42, test_size=0.2\n",
    ")\n",
    "train_masks, validation_masks, _, _ = train_test_split(\n",
    "    attention_mask, labels, random_state=42, test_size=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e02b13-e20d-4572-9665-bdda2bf19b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all data to PyTorch tensors\n",
    "train_inputs_tensors = torch.tensor(train_inputs)\n",
    "validation_inputs_tensors = torch.tensor(validation_inputs)\n",
    "\n",
    "train_labels_tensors = torch.tensor(train_labels, dtype=torch.long)\n",
    "validation_labels_tensors = torch.tensor(validation_labels, dtype=torch.long)\n",
    "\n",
    "train_masks_tensors = torch.tensor(train_masks)\n",
    "validation_masks_tensors = torch.tensor(validation_masks)\n",
    "\n",
    "# Create the TensorDataset\n",
    "train_dataset = TensorDataset(train_inputs_tensors, train_masks_tensors, train_labels_tensors)\n",
    "validation_dataset = TensorDataset(validation_inputs_tensors, validation_masks_tensors, validation_labels_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7796b7-e196-404e-b100-2ad4e50da0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# Create the training DataLoader\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    sampler=RandomSampler(train_dataset),  # Randomly select batches for training\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Create the validation DataLoader\n",
    "validation_dataloader = DataLoader(\n",
    "    validation_dataset,\n",
    "    sampler=SequentialSampler(validation_dataset),  # Process batches in order for validation\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb7a24e-805d-4cb9-89ef-4b59be529a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the number of output labels based on your problem\n",
    "# If you have two classes (e.g., good/bad quality), num_labels=2.\n",
    "num_labels = 2\n",
    "\n",
    "# Check if a GPU is available and use it\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# Load the pre-trained BERT model with a classification head\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=num_labels,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False\n",
    ")\n",
    "\n",
    "# Send the model to the GPU if available\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Model loaded and sent to {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832ea403-970d-4ef7-8611-08861fe4ac07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "epochs = 3\n",
    "learning_rate = 2e-5 \n",
    "epsilon = 1e-8\n",
    "\n",
    "# Get all of the model's parameters as a list of tuples.\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, eps=epsilon)\n",
    "\n",
    "# Calculate the total number of training steps\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5675e22-b947-4ffb-b1bf-a82601217793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose you have these counts\n",
    "num_relevant = 10000\n",
    "num_irrelevant = 384\n",
    "\n",
    "# Compute weights: total / (num_classes * class_count)\n",
    "weight_relevant = (num_relevant + num_irrelevant) / (2 * num_relevant)\n",
    "weight_irrelevant = (num_relevant + num_irrelevant) / (2 * num_irrelevant)\n",
    "\n",
    "class_weights = torch.tensor([weight_irrelevant,weight_relevant]).to(device)\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5531c395-8b16-4693-bd05-9ab7a7e66d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a custom loss function\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c702d3a6-24d2-4b54-be42-c8f635c74e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch + 1}/{epochs}')\n",
    "    print('Training...')\n",
    "\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Iterate over batches of data\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        # Unpack the batch and send tensors to the GPU\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "\n",
    "        # Clear any previously calculated gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Perform a forward pass\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            # labels=labels\n",
    "        )\n",
    "        \n",
    "        # Calculate loss and perform backpropagation\n",
    "        logits = outputs.logits\n",
    "        loss = loss_fn(logits, labels)\n",
    "        # loss = outputs.loss\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        num_misclassified_irrelevant = ((preds != labels) & (labels == 0)).sum().item()\n",
    "        num_misclassified_relevant = ((preds != labels) & (labels == 1)).sum().item()\n",
    "        # print(num_misclassified_irrelevant, num_misclassified_relevant)\n",
    "        \n",
    "        # Clip the norm of the gradients to prevent \"exploding gradients\"\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over the epoch\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    print(f'Average training loss: {avg_train_loss:.4f}')\n",
    "    \n",
    "    # Validation step\n",
    "    print('\\nValidating...')\n",
    "    model.eval()\n",
    "    total_eval_accuracy = 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "\n",
    "        # Tell PyTorch not to compute gradients\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "        logits = outputs.logits\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        labels = labels.to('cpu').numpy()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        predictions = np.argmax(logits, axis=1).flatten()\n",
    "        total_eval_accuracy += np.sum(predictions == labels)\n",
    "\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataset)\n",
    "    print(f'Validation Accuracy: {avg_val_accuracy:.4f}')\n",
    "\n",
    "print('\\nTraining complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c5105f-db6e-40f4-adae-20077717837d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define save directory\n",
    "save_directory = \"model\"\n",
    "\n",
    "# Save model\n",
    "model.save_pretrained(save_directory)\n",
    "\n",
    "# Save tokenizer (make sure it's the same one you used for training)\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "print(f\"Model saved to {save_directory}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
