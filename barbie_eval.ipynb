{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Please run this notebook in Google Colaboratory.\n",
        "\n",
        "Please upload your labelled testing csv to this folder:\n",
        "\n",
        "https://drive.google.com/drive/folders/1LISkmkab8S7DBECTFb65DqD3a3_GSeI8?usp=sharing\n",
        "\n",
        "Please note that the table should contain a `cleaned_text` and a `classification` column.\n",
        "\n",
        "|...|cleaned_text|classification|...|\n",
        "|:---:|:---:|:---:|:---:|\n",
        "|$\\vdots$|review 1|label 1|$\\vdots$|\n",
        "|$\\vdots$|$\\vdots$|$\\vdots$|$\\vdots$|\n"
      ],
      "metadata": {
        "id": "lJCpewZkzTr9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preamble"
      ],
      "metadata": {
        "id": "GAP3UpH-xAvm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OX3ovoe5w66V"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, classification_report, confusion_matrix\n",
        ")\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gdown\n",
        "import glob\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "This folder cointains our model and tokenizer\n",
        "'''\n",
        "model_folder_id = '1mfbSHZ8pVC4WOvWPCDma-8n4A25dtty2'\n",
        "gdown.download_folder(id=model_folder_id,output=\"model\",quiet=False,use_cookies=False)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "kybrf5iTxpRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "This folder should contain your testing files\n",
        "'''\n",
        "test_folder_id = '1LISkmkab8S7DBECTFb65DqD3a3_GSeI8'\n",
        "gdown.download_folder(id=test_folder_id,output=\"test\",quiet=False,use_cookies=False)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "f7Gpka7Hy7Kp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "downloaded_folder = \"test\"\n",
        "\n",
        "csv_files = glob.glob(os.path.join(downloaded_folder, \"*.csv\"))\n",
        "\n",
        "if len(csv_files) != 1:\n",
        "    raise ValueError(f\"Expected 1 CSV in '{downloaded_folder}', found {len(csv_files)}\")\n",
        "\n",
        "csv_file_path = csv_files[0]\n",
        "new_name = os.path.join(downloaded_folder, \"reviews.csv\")\n",
        "os.rename(csv_file_path, new_name)\n",
        "\n",
        "print(f\"CSV downloaded and renamed to: {new_name}\")"
      ],
      "metadata": {
        "id": "dvNLoggHdr0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "l94f_9kD7CBY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Data\n",
        "df = pd.read_csv('/content/test/reviews.csv')\n",
        "\n",
        "# Extract text and labels\n",
        "texts = df['cleaned_text'].tolist()\n",
        "labels = df['classification'].tolist()  # assuming 0/1 integers\n",
        "\n",
        "# Load tokenizer\n",
        "save_directory = '/content/model'\n",
        "tokenizer = BertTokenizer.from_pretrained(save_directory)\n",
        "\n",
        "# Tokenize the dataset\n",
        "encoded_dict = tokenizer.batch_encode_plus(\n",
        "    texts,\n",
        "    add_special_tokens=True,      # Add '[CLS]' and '[SEP]'\n",
        "    max_length=128,        # Pad and truncate all reviews\n",
        "    padding='max_length',         # Pad to the max_length\n",
        "    truncation=True,              # Truncate sequences to max_length\n",
        "    return_attention_mask=True,   # Return attention mask\n",
        "    return_tensors='pt',          # Return PyTorch tensors\n",
        ")\n",
        "\n",
        "# Convert labels to tensor\n",
        "labels_tensor = torch.tensor(labels)\n",
        "\n",
        "# Wrap everything into a TensorDataset and DataLoader\n",
        "dataset = TensorDataset(encoded_dict['input_ids'], encoded_dict['attention_mask'], labels_tensor)\n",
        "dataloader = DataLoader(dataset, batch_size=32)  # adjust batch size as needed\n"
      ],
      "metadata": {
        "id": "V22-n4gT7eHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "0nlwOz4b-qVY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model from the directory\n",
        "model = BertForSequenceClassification.from_pretrained(save_directory)\n",
        "\n",
        "# Move the model to the correct device\n",
        "model.to(device)\n",
        "model.eval() # Set the model to evaluation mode\n",
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        "y_prob = []\n",
        "\n",
        "# Make a prediction\n",
        "with torch.no_grad():\n",
        "    for batch in dataloader:\n",
        "        input_ids = batch[0].to(device)\n",
        "        attention_mask = batch[1].to(device)\n",
        "        labels_batch = batch[2].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Softmax to get probabilities\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "        preds = torch.argmax(probs, dim=1)\n",
        "\n",
        "        y_true.extend(labels_batch.cpu().numpy())\n",
        "        y_pred.extend(preds.cpu().numpy())\n",
        "        y_prob.extend(probs[:, 1].cpu().numpy())  # prob of class \"Relevant\" (index 1)"
      ],
      "metadata": {
        "id": "wTx3F6LjzqSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "kfpa_mD7-yoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic metrics\n",
        "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
        "print(\"Precision:\", precision_score(y_true, y_pred, zero_division=0))\n",
        "print(\"Recall:\", recall_score(y_true, y_pred, zero_division=0))\n",
        "print(\"F1:\", f1_score(y_true, y_pred, zero_division=0))\n",
        "print(\"ROC AUC:\", roc_auc_score(y_true, y_prob))\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true, y_pred, zero_division=0))\n",
        "\n",
        "# Confusion matrix\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_true, y_pred))"
      ],
      "metadata": {
        "id": "htLmjczQ-2Lh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}